import torch
import torch.onnx
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import os

def convert_distilbert_to_onnx():
    """å°†DistilBERTä¸­æ–‡æ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼ç”¨äºæƒ…æ„Ÿåˆ†æ"""
    
    print("ğŸ¯ DistilBERTä¸­æ–‡æ¨¡å‹è½¬ONNX")
    print("=" * 50)
    
    # ===== 1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ =====
    model_path = "./distilbert-base-zh-cased"
    if not os.path.exists(model_path):
        print("âŒ æœªæ‰¾åˆ°DistilBERTæ¨¡å‹æ–‡ä»¶")
        print(f"è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨äº: {model_path}")
        return False
    
    # ===== 2. åŠ è½½tokenizer =====
    print("ğŸ“¥ åŠ è½½tokenizer...")
    try:
        tokenizer = DistilBertTokenizer.from_pretrained(model_path)
        print("âœ… TokenizeråŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ TokenizeråŠ è½½å¤±è´¥: {e}")
        return False
    
    # ===== 3. åˆ›å»ºåˆ†ç±»æ¨¡å‹ =====
    print("ğŸ”§ åˆ›å»ºæƒ…æ„Ÿåˆ†ææ¨¡å‹...")
    try:
        # åˆ›å»ºä¸€ä¸ª3åˆ†ç±»çš„æƒ…æ„Ÿåˆ†ææ¨¡å‹
        model = DistilBertForSequenceClassification.from_pretrained(
            model_path,
            num_labels=3,  # è´Ÿé¢ã€ä¸­æ€§ã€æ­£é¢
            id2label={0: "è´Ÿé¢", 1: "ä¸­æ€§", 2: "æ­£é¢"},
            label2id={"è´Ÿé¢": 0, "ä¸­æ€§": 1, "æ­£é¢": 2}
        )
        model.eval()
        print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # ===== 4. å‡†å¤‡è™šæ‹Ÿè¾“å…¥ =====
    print("ğŸ”§ å‡†å¤‡è™šæ‹Ÿè¾“å…¥æ•°æ®...")
    dummy_text = "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œå¿ƒæƒ…ä¸é”™"
    max_length = 128
    
    # ä½¿ç”¨tokenizerå¤„ç†è™šæ‹Ÿè¾“å…¥
    inputs = tokenizer(
        dummy_text,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=max_length
    )
    
    dummy_input_ids = inputs['input_ids']
    dummy_attention_mask = inputs['attention_mask']
    
    print(f"  - è¾“å…¥æ–‡æœ¬: {dummy_text}")
    print(f"  - åºåˆ—é•¿åº¦: {max_length}")
    print(f"  - input_idså½¢çŠ¶: {dummy_input_ids.shape}")
    print(f"  - attention_maskå½¢çŠ¶: {dummy_attention_mask.shape}")
    
    # ===== 5. å®šä¹‰è¾“å…¥å’Œè¾“å‡ºåç§° =====
    input_names = ['input_ids', 'attention_mask']
    output_names = ['logits']
    
    # ===== 6. å®šä¹‰åŠ¨æ€è½´ =====
    dynamic_axes = {
        'input_ids': {0: 'batch_size', 1: 'sequence_length'},
        'attention_mask': {0: 'batch_size', 1: 'sequence_length'},
        'logits': {0: 'batch_size'}
    }
    
    # ===== 7. å¯¼å‡ºONNXæ¨¡å‹ =====
    onnx_path = "./distilbert-base-zh-cased/distilbert_emotion_analysis.onnx"
    print(f"ğŸš€ å¼€å§‹å¯¼å‡ºONNXæ¨¡å‹åˆ° {onnx_path}...")
    
    try:
        torch.onnx.export(
            model,
            (dummy_input_ids, dummy_attention_mask),
            onnx_path,
            export_params=True,
            opset_version=14,
            do_constant_folding=True,
            input_names=input_names,
            output_names=output_names,
            dynamic_axes=dynamic_axes,
            verbose=False
        )
        print("âœ… ONNXæ¨¡å‹å¯¼å‡ºæˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ ONNXå¯¼å‡ºå¤±è´¥: {e}")
        return False
    
    # ===== 8. éªŒè¯ONNXæ¨¡å‹ =====
    print("ğŸ” éªŒè¯ONNXæ¨¡å‹...")
    try:
        import onnxruntime as ort
        
        # åŠ è½½ONNXæ¨¡å‹
        session = ort.InferenceSession(onnx_path, providers=["CPUExecutionProvider"])
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        onnx_inputs = {
            'input_ids': dummy_input_ids.numpy(),
            'attention_mask': dummy_attention_mask.numpy()
        }
        
        # è¿è¡Œæ¨ç†
        onnx_outputs = session.run(None, onnx_inputs)
        
        # ä¸PyTorchæ¨¡å‹è¾“å‡ºå¯¹æ¯”
        with torch.no_grad():
            pytorch_outputs = model(dummy_input_ids, dummy_attention_mask)
        
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸€è‡´
        pytorch_logits = pytorch_outputs.logits.numpy()
        onnx_logits = onnx_outputs[0]
        
        max_diff = abs(pytorch_logits - onnx_logits).max()
        print(f"ğŸ“Š PyTorchä¸ONNXè¾“å‡ºæœ€å¤§å·®å¼‚: {max_diff:.6f}")
        
        if max_diff < 1e-4:
            print("âœ… ONNXæ¨¡å‹éªŒè¯é€šè¿‡ï¼")
        else:
            print("âš ï¸  ONNXæ¨¡å‹è¾“å‡ºä¸PyTorchæœ‰è¾ƒå¤§å·®å¼‚")
        
        # ===== 9. è¾“å‡ºæ¨¡å‹ä¿¡æ¯ =====
        print("\nğŸ“‹ ONNXæ¨¡å‹ä¿¡æ¯:")
        print(f"  - æ–‡ä»¶è·¯å¾„: {onnx_path}")
        print(f"  - æ–‡ä»¶å¤§å°: {os.path.getsize(onnx_path) / (1024*1024):.2f} MB")
        print(f"  - è¾“å…¥æ•°é‡: {len(session.get_inputs())}")
        print(f"  - è¾“å‡ºæ•°é‡: {len(session.get_outputs())}")
        
        for i, input_info in enumerate(session.get_inputs()):
            print(f"  - è¾“å…¥{i+1}: {input_info.name}, å½¢çŠ¶: {input_info.shape}, ç±»å‹: {input_info.type}")
        
        for i, output_info in enumerate(session.get_outputs()):
            print(f"  - è¾“å‡º{i+1}: {output_info.name}, å½¢çŠ¶: {output_info.shape}, ç±»å‹: {output_info.type}")
        
        # ===== 10. æµ‹è¯•æƒ…æ„Ÿåˆ†æ =====
        print("\nğŸ§ª æµ‹è¯•æƒ…æ„Ÿåˆ†æåŠŸèƒ½:")
        test_texts = [
            "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œå¿ƒæƒ…ç‰¹åˆ«æ£’ï¼",
            "è¿™éƒ¨ç”µå½±å¤ªç³Ÿç³•äº†ï¼Œå®Œå…¨æµªè´¹æ—¶é—´",
            "è¿˜å¯ä»¥å§ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„"
        ]
        
        id2label = {0: "è´Ÿé¢", 1: "ä¸­æ€§", 2: "æ­£é¢"}
        
        for text in test_texts:
            # Tokenize
            test_inputs = tokenizer(
                text,
                return_tensors="np",
                padding="max_length",
                truncation=True,
                max_length=max_length
            )
            
            # ONNXæ¨ç†
            test_onnx_inputs = {
                'input_ids': test_inputs['input_ids'],
                'attention_mask': test_inputs['attention_mask']
            }
            
            test_outputs = session.run(None, test_onnx_inputs)
            logits = test_outputs[0]
            
            # è·å–é¢„æµ‹ç»“æœ
            import numpy as np
            
            # æ‰‹åŠ¨å®ç°softmax
            def softmax(x):
                exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
                return exp_x / np.sum(exp_x, axis=1, keepdims=True)
            
            predicted_class = int(np.argmax(logits, axis=1)[0])
            predicted_label = id2label[predicted_class]
            probabilities = softmax(logits)[0]
            confidence = float(probabilities[predicted_class])
            
            print(f"  ğŸ“ \"{text}\" â†’ {predicted_label} (ç½®ä¿¡åº¦: {confidence:.4f})")
        
        return True
        
    except ImportError:
        print("âš ï¸  æœªå®‰è£…onnxruntimeï¼Œè·³è¿‡éªŒè¯æ­¥éª¤")
        print("   å¯è¿è¡Œ: pip install onnxruntime")
        return True
    except Exception as e:
        print(f"âŒ ONNXéªŒè¯å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = convert_distilbert_to_onnx()
    
    if success:
        print("\nğŸ‰ DistilBERTè½¬ONNXå®Œæˆï¼")
        print("ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨ distilbert_emotion_analysis.onnx è¿›è¡Œé«˜æ•ˆæ¨ç†äº†ï¼")
    else:
        print("\nğŸ’¡ æç¤º:")
        print("1. ç¡®ä¿DistilBERTæ¨¡å‹æ–‡ä»¶å­˜åœ¨äº './distilbert-base-zh-cased' ç›®å½•")
        print("2. ç¡®ä¿å·²å®‰è£…æ‰€éœ€ä¾èµ–: pip install torch transformers onnxruntime")