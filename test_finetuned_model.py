import torch
from transformers import BertTokenizer, BertForSequenceClassification
import sys

# æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
if len(sys.argv) < 2:
    print("ä½¿ç”¨æ–¹æ³•: python test_finetuned_model.py \"æ‚¨çš„æ–‡æœ¬å†…å®¹\"")
    print("ç¤ºä¾‹: python test_finetuned_model.py \"ä»Šå¤©å¤©æ°”çœŸå¥½ï¼\"")
    sys.exit(1)

# ä»å‘½ä»¤è¡Œå‚æ•°è·å–è¾“å…¥æ–‡æœ¬
input_text = sys.argv[1]

# ===== 1. åŠ è½½å¾®è°ƒåçš„æ¨¡å‹å’Œtokenizer =====
model_path = "./tinybert_emotion_finetuned"

try:
    tokenizer = BertTokenizer.from_pretrained(model_path)
    model = BertForSequenceClassification.from_pretrained(model_path)
    print("âœ… æˆåŠŸåŠ è½½å¾®è°ƒåçš„æ¨¡å‹")
except Exception as e:
    print(f"âŒ æ— æ³•åŠ è½½å¾®è°ƒåçš„æ¨¡å‹: {e}")
    print("è¯·å…ˆè¿è¡Œ finetune_tinybert.py è¿›è¡Œæ¨¡å‹å¾®è°ƒ")
    sys.exit(1)

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
model.eval()

# ===== 2. é¢„å¤„ç†è¾“å…¥æ–‡æœ¬ =====
inputs = tokenizer(
    input_text,
    return_tensors="pt",
    padding="max_length",
    truncation=True,
    max_length=128
)

# å°†è¾“å…¥ç§»åˆ°è®¾å¤‡ä¸Š
input_ids = inputs['input_ids'].to(device)
attention_mask = inputs['attention_mask'].to(device)
token_type_ids = inputs['token_type_ids'].to(device)

# ===== 3. æ¨¡å‹æ¨ç† =====
with torch.no_grad():
    outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        token_type_ids=token_type_ids
    )
    
    logits = outputs.logits
    probabilities = torch.softmax(logits, dim=1)
    predicted_class = torch.argmax(logits, dim=1).item()

# ===== 4. è§£æç»“æœ =====
id2label = {0: "è´Ÿé¢", 1: "ä¸­æ€§", 2: "æ­£é¢"}
label2emoji = {"è´Ÿé¢": "ğŸ˜", "ä¸­æ€§": "ğŸ˜", "æ­£é¢": "ğŸ˜Š"}

predicted_label = id2label[predicted_class]
predicted_emoji = label2emoji[predicted_label]

# ===== 5. è¾“å‡ºç»“æœ =====
print("=" * 60)
print(f"ğŸ“ è¾“å…¥æ–‡æœ¬: {input_text}")
print(f"{predicted_emoji} é¢„æµ‹æƒ…æ„Ÿ: {predicted_label}")
print(f"ğŸ¯ ç½®ä¿¡åº¦: {probabilities[0][predicted_class].item():.4f}")
print("=" * 60)

# æ˜¾ç¤ºæ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
print("ğŸ“Š å„ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ:")
for i, (label, prob) in enumerate(zip(["è´Ÿé¢", "ä¸­æ€§", "æ­£é¢"], probabilities[0])):
    emoji = label2emoji[label]
    bar_length = int(prob.item() * 20)  # 20ä¸ªå­—ç¬¦çš„è¿›åº¦æ¡
    bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
    print(f"  {emoji} {label}: {bar} {prob.item():.4f}")

print("=" * 60)