import torch
from transformers import AutoModel, AutoTokenizer
import onnx
import os

def main():
    print("å¼€å§‹è½¬æ¢TinyBERTæ¨¡å‹ä¸ºONNXæ ¼å¼...")
    
    # æ¨¡å‹é…ç½®
    model_name = "huawei-noah/TinyBERT_General_4L_312D"
    seq_len = 128  # åºåˆ—é•¿åº¦
    
    print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_name}")
    
    # åŠ è½½tokenizerå’Œæ¨¡å‹
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        print("âœ… æ¨¡å‹å’ŒtokenizeråŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # å‡†å¤‡ç¤ºä¾‹è¾“å…¥
    print(f"ğŸ”§ å‡†å¤‡ç¤ºä¾‹è¾“å…¥ (åºåˆ—é•¿åº¦: {seq_len})...")
    dummy_inputs = tokenizer(
        "ç¤ºä¾‹æ–‡æœ¬", 
        max_length=seq_len, 
        padding="max_length", 
        truncation=True, 
        return_tensors="pt"
    )
    
    print(f"  - input_ids shape: {dummy_inputs['input_ids'].shape}")
    print(f"  - attention_mask shape: {dummy_inputs['attention_mask'].shape}")
    print(f"  - token_type_ids shape: {dummy_inputs['token_type_ids'].shape}")
    
    # è¾“å‡ºè·¯å¾„
    output_path = "./TinyBERT/tinybert_emotion_analysis_128.onnx"
    
    print(f"ğŸš€ å¼€å§‹å¯¼å‡ºONNXæ¨¡å‹åˆ°: {output_path}")
    
    try:
        torch.onnx.export(
            model,
            (dummy_inputs["input_ids"], dummy_inputs["attention_mask"], dummy_inputs["token_type_ids"]),
            output_path,
            input_names=["input_ids", "attention_mask", "token_type_ids"],
            output_names=["last_hidden_state"],
            dynamic_axes={  # æ”¯æŒåŠ¨æ€é•¿åº¦
                "input_ids": {1: "seq_len"},
                "attention_mask": {1: "seq_len"},
                "token_type_ids": {1: "seq_len"},
            },
            opset_version=14
        )
        print("âœ… ONNXæ¨¡å‹å¯¼å‡ºæˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ ONNXå¯¼å‡ºå¤±è´¥: {e}")
        return
    
    # éªŒè¯ONNXæ¨¡å‹
    print("ğŸ” éªŒè¯ONNXæ¨¡å‹...")
    try:
        onnx_model = onnx.load(output_path)
        onnx.checker.check_model(onnx_model)
        print("âœ… ONNXæ¨¡å‹éªŒè¯é€šè¿‡")
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
        print(f"\nğŸ“‹ ONNXæ¨¡å‹ä¿¡æ¯:")
        print(f"  - æ–‡ä»¶è·¯å¾„: {output_path}")
        print(f"  - æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        print(f"  - è¾“å…¥æ•°é‡: 3 (input_ids, attention_mask, token_type_ids)")
        print(f"  - è¾“å‡ºæ•°é‡: 1 (last_hidden_state)")
        print(f"  - æ”¯æŒåŠ¨æ€åºåˆ—é•¿åº¦: æ˜¯")
        print(f"  - æœ€å¤§åºåˆ—é•¿åº¦: {seq_len}")
        
    except Exception as e:
        print(f"âŒ ONNXæ¨¡å‹éªŒè¯å¤±è´¥: {e}")
        return
    
    print("\nğŸ‰ TinyBERTè½¬ONNXå®Œæˆï¼")
    print(f"ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨ {output_path} è¿›è¡Œæ¨ç†äº†ï¼")
    print("\næ³¨æ„: æ­¤æ¨¡å‹è¾“å‡ºçš„æ˜¯last_hidden_stateï¼Œå¦‚éœ€æƒ…æ„Ÿåˆ†æï¼Œéœ€è¦é¢å¤–çš„åˆ†ç±»å±‚ã€‚")

if __name__ == "__main__":
    main()