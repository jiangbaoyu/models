# -*- coding: utf-8 -*-
"""
make_new_train.py
æ„å»º new_train.jsonï¼š
- è¯»å– CSV/JSONï¼ˆå¯å¤šæ–‡ä»¶ï¼‰
- æ¸…æ´—ã€å»é‡ã€é•¿åº¦è£å‰ª
- æœ‰/æ— æ ‡ç­¾ä¸¤ç§æ¨¡å¼ï¼š
  (a) æœ‰æ ‡ç­¾ï¼šç”¨ç°æœ‰æ¨¡å‹åšç½®ä¿¡åº¦è¯„ä¼°ï¼ŒæŒ–æ˜éš¾æ ·æœ¬ï¼ˆä½ margin / é«˜ entropyï¼‰
  (b) æ— æ ‡ç­¾ï¼šè‡ªåŠ¨æ‰“æ ‡ï¼Œå¹¶è¿‡æ»¤ä½ç½®ä¿¡æ ·æœ¬
- ç±»å‡è¡¡ï¼š--target-per-class
- å°ç±»è½»é‡å¢å¼ºï¼ˆå¯é€‰ï¼‰ï¼š--augment
- å¯¼å‡ºï¼š
    new_train.jsonï¼ˆè®­ç»ƒé›†ï¼‰
    review_hard.jsonï¼ˆå»ºè®®äººå·¥å¤æ ¸ï¼‰
    stats.jsonï¼ˆæ•°æ®ä¸ç­›é€‰ç»Ÿè®¡ï¼‰
"""

import os, json, argparse, re, glob, uuid
from collections import Counter, defaultdict
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification


# =============== æ–‡æœ¬æ¸…æ´— ===============
_URL = re.compile(r'https?://\S+|www\.\S+')
_AT = re.compile(r'@\S+')
_TOPIC = re.compile(r'#([^#]+)#')
_SPACE = re.compile(r'\s+')

def clean_text(s: str) -> str:
    s = str(s)
    s = _URL.sub(' ', s)
    s = _AT.sub(' ', s)
    s = _topic_sub(s)
    s = s.replace('è½¬å‘å¾®åš', ' ').replace('æ¥è‡ª', ' ')
    s = _SPACE.sub(' ', s).strip()
    return s

def _topic_sub(s: str) -> str:
    return _TOPIC.sub(r'\1', s)


# =============== è¯»æ•°æ®ï¼ˆæ”¯æŒ CSV/JSONï¼Œå¤šæ–‡ä»¶ï¼‰ ===============
TYPICAL_LABELS = {
    "angry","anger","happy","joy","sad","depress","fear","neutral","surprise","disgust",
    "ç”Ÿæ°”","æ„¤æ€’","é«˜å…´","å¼€å¿ƒ","å¿«ä¹","ä¼¤å¿ƒ","éš¾è¿‡","ææƒ§","ä¸­æ€§","æƒŠè®¶","åŒæ¶"
}

def load_one(path: str) -> pd.DataFrame:
    if path.lower().endswith((".json", ".jsonl", ".txt")):
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        # å…¼å®¹ content / text
        rows = []
        for it in data:
            text = it.get("content", it.get("text", None))
            if text is None:
                continue
            lab = it.get("label", None)
            rows.append({"text": text, "label": lab})
        df = pd.DataFrame(rows)
    else:
        df = pd.read_csv(path, sep=None, engine="python", header=None, keep_default_na=False)
        # çŒœæµ‹ label åˆ—
        def is_label_col(col) -> bool:
            vals = df[col].astype(str).str.strip().tolist()
            uniq = set(vals)
            if len(uniq & TYPICAL_LABELS) >= 3:
                return True
            try:
                nums = pd.to_numeric(vals, errors="coerce")
                u = set(nums.dropna().astype(int).unique().tolist())
                if 2 <= len(u) <= 20:
                    return True
            except Exception:
                pass
            return False

        label_cols = [c for c in df.columns if is_label_col(c)]
        if label_cols:
            lc = label_cols[0]
            text_cols = [c for c in df.columns if c != lc]
            text = df[text_cols].astype(str).apply(
                lambda r: " ".join([x for x in r if x and x.lower()!="nan"]), axis=1
            )
            df = pd.DataFrame({"text": text, "label": df[lc].astype(str)})
        else:
            # æ— æ³•è¯†åˆ«æ ‡ç­¾åˆ—ï¼Œå½“ä½œçº¯æ–‡æœ¬
            text = df.astype(str).apply(
                lambda r: " ".join([x for x in r if x and x.lower()!="nan"]), axis=1
            )
            df = pd.DataFrame({"text": text, "label": None})
    # æ¸…æ´—
    df["text"] = df["text"].astype(str).apply(clean_text)
    # å»ç©ºä¸é‡å¤
    df = df[df["text"].str.len() >= 1].drop_duplicates(subset=["text"]).reset_index(drop=True)
    return df

def load_many(paths: List[str]) -> pd.DataFrame:
    files = []
    for p in paths:
        files.extend(glob.glob(p))
    if not files:
        raise FileNotFoundError("æ‰¾ä¸åˆ°ä»»ä½•è¾“å…¥æ–‡ä»¶")
    parts = [load_one(p) for p in files]
    df = pd.concat(parts, axis=0).reset_index(drop=True)
    return df


# =============== æ¨¡å‹æ¨ç†ï¼ˆç½®ä¿¡åº¦/æ‰“æ ‡/éš¾æ ·æœ¬æŒ–æ˜ï¼‰ ===============
@torch.no_grad()
def model_scores(texts: List[str], tok, model, max_len: int, batch_size: int = 64) -> Tuple[np.ndarray, np.ndarray]:
    """è¿”å› logits(np.array [N,C]) ä¸ prob(np.array [N,C])"""
    device = next(model.parameters()).device
    all_logits = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        enc = tok(batch, truncation=True, max_length=max_len, return_tensors="pt", padding=True)
        enc = {k: v.to(device) for k, v in enc.items()}
        logits = model(**enc).logits
        all_logits.append(logits.cpu())
    logits = torch.cat(all_logits, dim=0).numpy()
    prob = torch.softmax(torch.tensor(logits), dim=-1).numpy()
    return logits, prob

def entropy(p: np.ndarray) -> np.ndarray:
    p = np.clip(p, 1e-8, 1.0)
    return -(p * np.log(p)).sum(axis=-1)

def margin(p: np.ndarray) -> np.ndarray:
    part = -np.sort(-p, axis=-1)  # é™åº
    return part[:,0] - part[:,1]


# =============== è½»é‡â€œå®‰å…¨â€å¢å¼ºï¼ˆå¯¹å°ç±»ï¼‰ ===============
ADV_WORDS = [
    ("å¾ˆ", ["å¾ˆ","ç‰¹åˆ«","æ ¼å¤–","ç›¸å½“"]),
    ("éå¸¸", ["éå¸¸","ååˆ†","æå…¶"]),
    ("æœ‰ç‚¹", ["æœ‰ç‚¹","æœ‰äº›","ç•¥å¾®"]),
]
TAIL_PUNC = ["ï¼","!!","ï¼ï¼ï¼","~","ï½ï½","~!","!~"]

def light_augment(text: str, max_len: int) -> str:
    s = text
    # 1) ç¨‹åº¦å‰¯è¯æ›¿æ¢ï¼ˆä¿æŒè¯­ä¹‰å¼ºåº¦ç›¸è¿‘ï¼‰
    for key, candidates in ADV_WORDS:
        if key in s and np.random.rand() < 0.5:
            s = s.replace(key, np.random.choice(candidates), 1)
    # 2) å¥å°¾æƒ…æ„Ÿç¬¦å·å¢å¼º
    if len(s) <= max_len and np.random.rand() < 0.4 and not s.endswith(("ã€‚","ï¼Ÿ","ï¼","!","~")):
        s = s + np.random.choice(TAIL_PUNC)
    # 3) è½»å¾®æ‹‰é•¿éŸ³
    if np.random.rand() < 0.3:
        s = re.sub(r"(å“ˆ|å•Š|å“¦|å‘€|å‘œ|å•¦){1}", lambda m: m.group(1) * np.random.randint(2,4), s, count=1)
    return s[:max_len]


# =============== ä¸»æµç¨‹ ===============
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--inputs", nargs="+", required=True, help="è¾“å…¥æ–‡ä»¶ï¼ˆCSV/JSONï¼›å¯å¤šæ–‡ä»¶/é€šé…ï¼‰")
    ap.add_argument("--model-dir", type=str, required=True, help="å·²è®­ç»ƒæ¨¡å‹ç›®å½•ï¼ˆå« tokenizerï¼‰")
    ap.add_argument("--out", type=str, default="new_train.json", help="è¾“å‡ºè®­ç»ƒé›† JSON")
    ap.add_argument("--review-out", type=str, default="review_hard.json", help="è¾“å‡ºå»ºè®®å¤æ ¸çš„éš¾æ ·æœ¬")
    ap.add_argument("--stats-out", type=str, default="stats.json", help="è¾“å‡ºç»Ÿè®¡ä¿¡æ¯")
    ap.add_argument("--max-len", type=int, default=256)
    ap.add_argument("--min-len", type=int, default=2)
    ap.add_argument("--target-per-class", type=int, default=4000, help="æ¯ç±»ç›®æ ‡æ ·æœ¬ä¸Šé™ï¼ˆå‡è¡¡ï¼‰")
    ap.add_argument("--batch-size", type=int, default=64)

    # æ¨¡å¼é€‰æ‹©
    ap.add_argument("--auto-label", action="store_true", help="æ— æ ‡ç­¾è¾“å…¥ï¼šç”¨æ¨¡å‹è‡ªåŠ¨æ‰“æ ‡")
    ap.add_argument("--conf-thres", type=float, default=0.5, help="è‡ªåŠ¨æ‰“æ ‡æ¨¡å¼ä¸‹çš„æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼")
    ap.add_argument("--mine-hard", action="store_true", help="æœ‰æ ‡ç­¾è¾“å…¥ï¼šæŒ–æ˜éš¾æ ·æœ¬ä»¥ä¾›äººå·¥å¤æ ¸")
    ap.add_argument("--hard-topk", type=int, default=2000, help="éš¾æ ·æœ¬å¯¼å‡ºæ•°é‡ä¸Šé™")
    ap.add_argument("--hard-margin", type=float, default=0.2, help="margin < é˜ˆå€¼è§†ä¸ºä¸ç¡®å®š")
    ap.add_argument("--augment", action="store_true", help="å¯¹å°ç±»åšè½»é‡å¢å¼º")
    ap.add_argument("--augment-max-per-class", type=int, default=1000, help="æ¯ç±»æœ€å¤šå¢å¼ºå¤šå°‘æ¡")
    ap.add_argument("--seed", type=int, default=42)

    args = ap.parse_args()
    np.random.seed(args.seed)

    # 1) è¯»å–
    df = load_many(args.inputs)
    df = df[df["text"].str.len() >= args.min_len].copy().reset_index(drop=True)

    # 2) æ¨¡å‹ä¸æ ‡ç­¾ç©ºé—´
    tokenizer = AutoTokenizer.from_pretrained(args.model_dir)
    model = AutoModelForSequenceClassification.from_pretrained(args.model_dir).eval()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    id2label = model.config.id2label
    # æŸäº›ç‰ˆæœ¬ id2label çš„é”®æ˜¯å­—ç¬¦ä¸²ä¸‹æ ‡
    id2label = {int(k): v for k, v in id2label.items()} if isinstance(list(model.config.id2label.keys())[0], str) else id2label
    label2id = {v: k for k, v in id2label.items()}
    known_labels = set(label2id.keys())

    # 3) æœ‰/æ— æ ‡ç­¾å¤„ç†
    has_label = df["label"].notna().sum() > 0
    auto_label_mode = args.auto_label or not has_label

    stats = {"total_in": len(df), "auto_label": bool(auto_label_mode)}

    if auto_label_mode:
        # è‡ªåŠ¨æ‰“æ ‡
        logits, prob = model_scores(df["text"].tolist(), tokenizer, model, args.max_len, args.batch_size)
        pred_id = prob.argmax(axis=-1)
        conf = prob.max(axis=-1)
        df["label"] = [id2label[int(i)] for i in pred_id]
        df["conf"] = conf
        # è¿‡æ»¤ä½ç½®ä¿¡
        keep = df["conf"] >= float(args.conf_thres)
        df_keep = df[keep].copy()
        stats.update({
            "auto_conf_thres": float(args.conf_thres),
            "auto_labeled": int(len(df_keep)),
            "auto_dropped_low_conf": int((~keep).sum())
        })
    else:
        # æœ‰æ ‡ç­¾ï¼šåªåšæ¨¡å‹è¯„ä¼°ï¼ŒæŒ–æ˜éš¾æ ·æœ¬
        # å°†éå—æ”¯æŒæ ‡ç­¾æ˜ å°„/å‰”é™¤
        mask_known = df["label"].astype(str).isin(known_labels)
        dropped_unknown = int((~mask_known).sum())
        df = df[mask_known].copy().reset_index(drop=True)
        stats["dropped_unknown_label"] = dropped_unknown

        logits, prob = model_scores(df["text"].tolist(), tokenizer, model, args.max_len, args.batch_size)
        pred_id = prob.argmax(axis=-1)
        pred_label = [id2label[int(i)] for i in pred_id]
        conf = prob.max(axis=-1)
        mg = margin(prob)
        ent = entropy(prob)
        df["pred_label"] = pred_label
        df["conf"] = conf
        df["margin"] = mg
        df["entropy"] = ent

        # éš¾æ ·æœ¬ï¼š1) é¢„æµ‹é”™çš„ï¼›2) margin < é˜ˆå€¼ï¼›æŒ‰ç†µ/ä½marginæ’åº
        hard = df[(df["pred_label"] != df["label"]) | (df["margin"] < float(args.hard_margin))].copy()
        hard["score"] = (1 - hard["margin"]) + hard["entropy"]
        hard = hard.sort_values(by="score", ascending=False).head(int(args.hard_topk))
        # å¯¼å‡ºå¤æ ¸é›†
        review = hard[["text","label","pred_label","conf","margin","entropy"]].to_dict("records")
        with open(args.review_out, "w", encoding="utf-8") as f:
            json.dump(review, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²å¯¼å‡ºå»ºè®®å¤æ ¸çš„éš¾æ ·æœ¬ï¼š{args.review_out}ï¼ˆ{len(review)} æ¡ï¼‰")

        df_keep = df[["text","label","conf","margin","entropy"]].copy()

    # 4) å‡è¡¡ & å¯é€‰å¢å¼º
    #   å…ˆè£å‰ªæ¯ç±»åˆ° target-per-class ä¸Šé™ï¼›ä¸è¶³çš„ç±»å¯åšå¢å¼ºæˆ–è¿‡é‡‡æ ·
    target = int(args.target_per_class)
    grouped = defaultdict(list)
    for _, row in df_keep.iterrows():
        grouped[str(row["label"])].append(row)

    # é¡ºåºæ‰“ä¹±ï¼Œä¿è¯éšæœºæ€§
    for k in grouped:
        np.random.shuffle(grouped[k])

    final_rows = []
    augment_count = defaultdict(int)

    for lab, rows in grouped.items():
        rows = list(rows)
        # è£å‰ªè¿‡å¤š
        if len(rows) > target:
            rows = rows[:target]
        # ä¸è¶³ï¼šå°è¯•å¢å¼ºæˆ–è¿‡é‡‡æ ·
        if len(rows) < target:
            need = target - len(rows)
            base = rows.copy()
            if args.augment and len(base) > 0:
                aug_max = min(int(args.augment_max_per_class), need)
                # è½»é‡å¢å¼º
                for i in range(aug_max):
                    src = base[i % len(base)]
                    new_text = light_augment(str(src["text"]), args.max_len)
                    rows.append({"text": new_text, "label": lab})
                augment_count[lab] += aug_max
                need -= aug_max
            # è‹¥ä»ä¸è¶³ï¼Œç®€å•è¿‡é‡‡æ ·è¡¥é½
            while need > 0 and len(base) > 0:
                src = base[need % len(base)]
                rows.append({"text": src["text"], "label": lab})
                need -= 1

        # æ”¶é›†
        for r in rows:
            final_rows.append({"id": str(uuid.uuid4()), "content": str(r["text"])[:args.max_len], "label": lab})

    # 5) å»é‡ï¼ˆå†æ¬¡ï¼‰
    df_out = pd.DataFrame(final_rows).drop_duplicates(subset=["content","label"]).reset_index(drop=True)

    # 6) å¯¼å‡º
    with open(args.out, "w", encoding="utf-8") as f:
        json.dump(df_out.to_dict("records"), f, ensure_ascii=False, indent=2)

    # 7) ç»Ÿè®¡
    dist = Counter(df_out["label"].tolist())
    stats.update({
        "after_total": int(len(df_out)),
        "class_dist": dict(dist),
        "target_per_class": target,
        "augment_used": bool(args.augment),
        "augment_count": {k:int(v) for k,v in augment_count.items()}
    })
    with open(args.stats_out, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print(f"âœ… new_train å¯¼å‡ºï¼š{args.out}ï¼ˆ{len(df_out)} æ¡ï¼‰")
    print(f"ğŸ“Š ç»Ÿè®¡ï¼š{args.stats_out}")
    print("ğŸ“Œ ç±»åˆ†å¸ƒï¼š", dict(dist))


if __name__ == "__main__":
    main()
