# export_to_onnx.py
# ç”¨é€”ï¼šæŠŠHFæƒ…æ„Ÿåˆ†ç±»æ¨¡å‹å¯¼å‡ºä¸ºONNXï¼ˆå¯é€‰å¼ºåˆ¶INT32è¾“å…¥ï¼‰ï¼Œå¹¶åšORTéªŒè¯ä¸å¯é€‰ç®€åŒ–
# ä¾èµ–ï¼špip install torch transformers onnx onnxruntime onnxsim -U

import os
import argparse
import json
import torch
import numpy as np

from transformers import AutoTokenizer, AutoModelForSequenceClassification


def export_onnx(model_dir: str,
                onnx_path: str,
                max_len: int = 256,
                opset: int = 13,
                force_int32_inputs: bool = False,
                simplify: bool = False):
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # 1) åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)
    model = AutoModelForSequenceClassification.from_pretrained(model_dir).to(device).eval()

    # 2) æ„é€ dummyè¾“å…¥ï¼ˆä¸­æ–‡æ ·ä¾‹ï¼‰
    dummy = tokenizer("ä»Šå¤©å¿ƒæƒ…å¾ˆå¥½ï¼Œæ•ˆç‡ä¹Ÿå¾ˆé«˜ï¼", return_tensors="pt", truncation=True, max_length=max_len)
    # åŠ¨æ€è½´éœ€è¦è‡³å°‘ä¸€äº›paddingç»´åº¦ï¼Œç¡®ä¿å¯¼å‡ºå›¾å¯æ‰©å±•
    if dummy["input_ids"].shape[1] < 8:
        dummy = tokenizer("ä»Šå¤©å¿ƒæƒ…å¾ˆå¥½ï¼Œæ•ˆç‡ä¹Ÿå¾ˆé«˜ï¼"*8, return_tensors="pt",
                          truncation=True, max_length=max_len)

    # å¯é€‰ï¼šæŠŠè¾“å…¥castä¸ºint32ï¼ˆå¤šæ•°NLPæ¨¡å‹é»˜è®¤æ˜¯int64ï¼ŒMSLiteå¸¸å¯¹int64ä¸å‹å¥½ï¼‰
    if force_int32_inputs:
        for k in ["input_ids", "attention_mask", "token_type_ids"]:
            if k in dummy:
                dummy[k] = dummy[k].to(torch.int32)
        input_dtypes = {"input_ids": "INT32", "attention_mask": "INT32",
                        "token_type_ids": "INT32" if "token_type_ids" in dummy else None}
    else:
        input_dtypes = {"input_ids": "INT64", "attention_mask": "INT64",
                        "token_type_ids": "INT64" if "token_type_ids" in dummy else None}

    # ç§»åŠ¨åˆ°è®¾å¤‡
    dummy = {k: v.to(device) for k, v in dummy.items()}

    # 3) å‡†å¤‡å¯¼å‡º
    os.makedirs(os.path.dirname(onnx_path) or ".", exist_ok=True)
    input_names = ["input_ids", "attention_mask"]
    inputs = [dummy["input_ids"], dummy["attention_mask"]]
    dynamic_axes = {"input_ids": {0: "batch", 1: "seq"},
                    "attention_mask": {0: "batch", 1: "seq"},
                    "logits": {0: "batch"}}

    # å¯¹æœ‰token_type_idsçš„æ¨¡å‹ï¼ˆéƒ¨åˆ†ä¸­æ–‡BERTç³»ï¼‰ä¸€å¹¶å¯¼å‡º
    if "token_type_ids" in dummy:
        input_names.append("token_type_ids")
        inputs.append(dummy["token_type_ids"])
        dynamic_axes["token_type_ids"] = {0: "batch", 1: "seq"}

    # 4) torch.onnx.export
    with torch.no_grad():
        torch.onnx.export(
            model,
            tuple(inputs),
            onnx_path,
            input_names=input_names,
            output_names=["logits"],
            dynamic_axes=dynamic_axes,
            opset_version=opset,
            do_constant_folding=True
        )

    print(f"âœ… ONNXå¯¼å‡ºæˆåŠŸ: {onnx_path}")

    # 5) å¯é€‰ï¼šonnx-simplifier ç®€åŒ–
    if simplify:
        try:
            import onnx
            from onnxsim import simplify as onnx_simplify
            onnx_model = onnx.load(onnx_path)
            model_simplified, check = onnx_simplify(onnx_model, dynamic_input_shape=True)
            assert check, "ONNX Simplifier æ ¡éªŒå¤±è´¥"
            onnx.save(model_simplified, onnx_path)
            print("âœ… ONNX æ¨¡å‹ç®€åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ ç®€åŒ–å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰ï¼š{e}")

    # 6) ORTéªŒè¯ï¼ˆå¯¹æ¯”PyTorchè¾“å‡ºï¼‰
    try:
        import onnxruntime as ort

        sess_opt = ort.SessionOptions()
        sess = ort.InferenceSession(onnx_path, sess_options=sess_opt, providers=["CPUExecutionProvider"])

        # å‡†å¤‡ä¸€ç»„æµ‹è¯•å¥
        test_texts = [
            "ä»Šå¤©å¿ƒæƒ…å¾ˆå¥½ï¼Œæ•ˆç‡ä¹Ÿå¾ˆé«˜ï¼",
            "çœŸæ˜¯æ°”æ­»æˆ‘äº†ï¼ŒæœåŠ¡å¤ªå·®ã€‚",
            "ä¸€èˆ¬èˆ¬å§ï¼Œæ²¡ä»€ä¹ˆæ„Ÿè§‰ã€‚",
            "æœ‰ç‚¹å®³æ€•ï¼Œå¿ƒé‡Œç›´æ‰“é¼“ã€‚",
        ]
        enc = tokenizer(test_texts, return_tensors="pt", truncation=True, padding=True, max_length=max_len)
        enc_torch = {k: v.to(device) for k, v in enc.items()}
        with torch.no_grad():
            pt_logits = model(**enc_torch).logits.detach().cpu().numpy()

        # ORTéœ€è¦numpyè¾“å…¥ï¼›ä¸”éœ€ä¸å¯¼å‡ºæ—¶dtypeä¸€è‡´
        ort_inputs = {}
        for name in input_names:
            arr = enc[name].cpu().numpy()
            if force_int32_inputs:
                arr = arr.astype(np.int32, copy=False)
            else:
                # æŸäº›tokenizerå¯èƒ½ç»™int64ï¼Œæœ¬å°±æ»¡è¶³
                arr = arr.astype(np.int64, copy=False)
            ort_inputs[name] = arr

        ort_logits = sess.run(["logits"], ort_inputs)[0]

        # å½’ä¸€åŒ–å¯¹æ¯”ï¼ˆå…è®¸æå°æ•°å€¼è¯¯å·®ï¼‰
        pt_prob = (torch.softmax(torch.tensor(pt_logits), dim=-1)).numpy()
        ort_prob = (torch.softmax(torch.tensor(ort_logits), dim=-1)).numpy()
        max_diff = float(np.max(np.abs(pt_prob - ort_prob)))
        print(f"ğŸ” ORTå¯¹é½æ£€éªŒ: max|softmax(pt)-softmax(ort)| = {max_diff:.6f}")
        assert max_diff < 1e-4, "ORTä¸PyTorchè¾“å‡ºå·®å¼‚åå¤§ï¼Œè¯·æ£€æŸ¥å¯¼å‡º/ç®€åŒ–æ­¥éª¤"

        # æ‰“å°IOä¸dtypeä¿¡æ¯
        print("ğŸ“ ONNX Inputs:")
        for i, inp in enumerate(sess.get_inputs()):
            print(f"  - {i}: name={inp.name}, shape={inp.shape}, dtype={inp.type}")
        out = sess.get_outputs()[0]
        print(f"ğŸ“ ONNX Output: name={out.name}, shape={out.shape}, dtype={out.type}")

        # å±•ç¤ºä¸€æ¡é¢„æµ‹
        id2label = model.config.id2label
        sample_pred = ort_logits.argmax(axis=-1)[0]
        print(f"ğŸ§ª ç¤ºä¾‹é¢„æµ‹: \"{test_texts[0]}\" -> {id2label[int(sample_pred)]}")

        # ä¿å­˜å…ƒä¿¡æ¯
        meta = {
            "model_dir": model_dir,
            "onnx_path": onnx_path,
            "max_len": max_len,
            "opset": opset,
            "inputs_dtype": input_dtypes,
            "num_labels": int(model.config.num_labels),
            "id2label": {int(k): v for k, v in model.config.id2label.items()} if isinstance(list(model.config.id2label.keys())[0], int)
                        else {int(k): v for k, v in model.config.id2label.items()}
        }
        with open(os.path.splitext(onnx_path)[0] + ".meta.json", "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
        print(f"âœ… å…ƒä¿¡æ¯å·²å†™å…¥: {os.path.splitext(onnx_path)[0]}.meta.json")

    except Exception as e:
        print(f"âš ï¸ ORTéªŒè¯é˜¶æ®µå‡ºç°é—®é¢˜ï¼ˆå¯å…ˆå¿½ç•¥å†æ’æŸ¥ï¼‰ï¼š{e}")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--model", type=str, required=True, help="HFæ¨¡å‹ç›®å½•ï¼ˆå¦‚ ./runs/distilbert_balanced20k_retrainï¼‰")
    ap.add_argument("--onnx", type=str, required=True, help="å¯¼å‡ºçš„ONNXè·¯å¾„ï¼ˆå¦‚ ./onnx/distilbert_emotion_256.onnxï¼‰")
    ap.add_argument("--max-len", type=int, default=256)
    ap.add_argument("--opset", type=int, default=13)
    ap.add_argument("--force-int32-inputs", action="store_true", help="å°†è¾“å…¥å¼ é‡dtypeå¼ºåˆ¶ä¸ºINT32ï¼ˆä¾¿äºåç»­MSLiteè½¬æ¢ï¼‰")
    ap.add_argument("--simplify", action="store_true", help="å¯¼å‡ºåç”¨ onnx-simplifier ç®€åŒ–æ¨¡å‹")
    args = ap.parse_args()

    export_onnx(
        model_dir=args.model,
        onnx_path=args.onnx,
        max_len=args.max_len,
        opset=args.opset,
        force_int32_inputs=args.force_int32_inputs,
        simplify=args.simplify
    )


if __name__ == "__main__":
    main()
