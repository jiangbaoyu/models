import platform
import psutil
import torch

def bytes_to_gb(bytes_val):
    return bytes_val / 1024 ** 3

print("="*40)
print("ğŸ” è®­ç»ƒç¯å¢ƒæ£€æµ‹")
print("="*40)

# ç³»ç»Ÿä¿¡æ¯
print(f"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()} ({platform.version()})")
print(f"Python ç‰ˆæœ¬: {platform.python_version()}")

# CPU ä¿¡æ¯
cpu_name = platform.processor() or "æœªçŸ¥CPU"
print(f"CPU å‹å·: {cpu_name}")
print(f"CPU æ ¸å¿ƒæ•°: {psutil.cpu_count(logical=False)} ç‰©ç† / {psutil.cpu_count(logical=True)} é€»è¾‘")

# å†…å­˜ä¿¡æ¯
mem = psutil.virtual_memory()
print(f"å†…å­˜æ€»é‡: {bytes_to_gb(mem.total):.2f} GB")

# GPU ä¿¡æ¯
if torch.cuda.is_available():
    print(f"æ£€æµ‹åˆ° {torch.cuda.device_count()} å— GPU")
    for i in range(torch.cuda.device_count()):
        prop = torch.cuda.get_device_properties(i)
        print(f"  GPU {i}: {prop.name}, æ˜¾å­˜: {bytes_to_gb(prop.total_memory):.2f} GB")
    print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
else:
    print("âš  æœªæ£€æµ‹åˆ°å¯ç”¨ GPUï¼ˆCUDA ä¸å¯ç”¨ï¼‰")

print("="*40)