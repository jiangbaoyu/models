import onnxruntime as ort
import numpy as np
import time
from transformers import BertTokenizer, DistilBertTokenizer
import sys

# æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
if len(sys.argv) < 2:
    print("ä½¿ç”¨æ–¹æ³•: python compare_models.py \"æ‚¨çš„æ–‡æœ¬å†…å®¹\"")
    print("ç¤ºä¾‹: python compare_models.py \"ä»Šå¤©å¤©æ°”çœŸå¥½ï¼\"")
    sys.exit(1)

# ä»å‘½ä»¤è¡Œå‚æ•°è·å–è¾“å…¥æ–‡æœ¬
input_text = sys.argv[1]

print("=" * 80)
print("ğŸ”¬ TinyBERT vs DistilBERT ONNX æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
print("=" * 80)
print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬: {input_text}")
print("=" * 80)

# ===== å®šä¹‰ç±»åˆ«æ ‡ç­¾ =====
id2label = {0: "è´Ÿé¢", 1: "ä¸­æ€§", 2: "æ­£é¢"}
label2emoji = {"è´Ÿé¢": "ğŸ˜", "ä¸­æ€§": "ğŸ˜", "æ­£é¢": "ğŸ˜Š"}
label2color = {"è´Ÿé¢": "ğŸ”´", "ä¸­æ€§": "ğŸŸ¡", "æ­£é¢": "ğŸŸ¢"}

def softmax(x):
    """è®¡ç®—softmaxæ¦‚ç‡"""
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

def test_model(model_name, model_path, tokenizer_path, tokenizer_class, max_length=128):
    """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
    print(f"\nğŸ§ª æµ‹è¯• {model_name} æ¨¡å‹...")
    
    # åŠ è½½tokenizer
    try:
        tokenizer = tokenizer_class.from_pretrained(tokenizer_path)
        print(f"âœ… {model_name} tokenizeråŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ {model_name} tokenizeråŠ è½½å¤±è´¥: {e}")
        return None
    
    # åŠ è½½ONNXæ¨¡å‹
    try:
        session = ort.InferenceSession(model_path, providers=["CPUExecutionProvider"])
        print(f"âœ… {model_name} ONNXæ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ {model_name} ONNXæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None
    
    # Tokenizeè¾“å…¥æ–‡æœ¬
    inputs = tokenizer(
        input_text,
        return_tensors="np",
        padding="max_length",
        truncation=True,
        max_length=max_length
    )
    
    # å‡†å¤‡ONNXè¾“å…¥
    if model_name == "TinyBERT":
        onnx_inputs = {
            "input_ids": inputs["input_ids"],
            "attention_mask": inputs["attention_mask"],
            "token_type_ids": inputs["token_type_ids"]
        }
    else:  # DistilBERT
        onnx_inputs = {
            "input_ids": inputs["input_ids"],
            "attention_mask": inputs["attention_mask"]
        }
    
    # å•æ¬¡æ¨ç†æµ‹è¯•
    try:
        outputs = session.run(None, onnx_inputs)
        if model_name == "TinyBERT":
            # TinyBERTè¾“å‡ºlast_hidden_stateï¼Œéœ€è¦æ¨¡æ‹Ÿåˆ†ç±»
            last_hidden_state = outputs[0]  # shape: [1, seq_len, 312]
            cls_vector = last_hidden_state[0, 0, :]  # å–CLS tokençš„å‘é‡
            # æ¨¡æ‹Ÿç®€å•çš„åˆ†ç±»é€»è¾‘ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼‰
            logits = np.array([[np.mean(cls_vector[:100]), np.mean(cls_vector[100:200]), np.mean(cls_vector[200:])]])
        else:
            logits = outputs[0]  # shape: [1, 3]
        print(f"âœ… {model_name} æ¨ç†å®Œæˆ")
    except Exception as e:
        print(f"âŒ {model_name} æ¨ç†å¤±è´¥: {e}")
        return None
    
    # è®¡ç®—é¢„æµ‹ç»“æœ
    probabilities = softmax(logits)[0]  # shape: [3]
    predicted_class = int(np.argmax(logits, axis=1)[0])
    predicted_label = id2label[predicted_class]
    predicted_emoji = label2emoji[predicted_label]
    predicted_color = label2color[predicted_label]
    confidence = float(probabilities[predicted_class])
    
    # æ€§èƒ½æµ‹è¯•
    print(f"âš¡ {model_name} æ€§èƒ½æµ‹è¯• (è¿è¡Œ100æ¬¡æ¨ç†)...")
    start_time = time.time()
    
    for _ in range(100):
        session.run(None, onnx_inputs)
    
    end_time = time.time()
    avg_time = (end_time - start_time) / 100 * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
    
    return {
        "model_name": model_name,
        "predicted_label": predicted_label,
        "predicted_emoji": predicted_emoji,
        "predicted_color": predicted_color,
        "confidence": confidence,
        "probabilities": probabilities,
        "avg_time": avg_time,
        "logits": logits[0],
        "token_count": np.sum(inputs['attention_mask'])
    }

# ===== æµ‹è¯•ä¸¤ä¸ªæ¨¡å‹ =====
results = []

# æµ‹è¯•TinyBERT (æ–°ç‰ˆæœ¬)
tinybert_result = test_model(
    "TinyBERT",
    "./TinyBERT/tinybert_emotion_analysis_128.onnx",
    "huawei-noah/TinyBERT_General_4L_312D",
    BertTokenizer
)
if tinybert_result:
    results.append(tinybert_result)

# æµ‹è¯•DistilBERT
distilbert_result = test_model(
    "DistilBERT",
    "./distilbert-base-zh-cased/distilbert_emotion_analysis.onnx",
    "./distilbert-base-zh-cased",
    DistilBertTokenizer
)
if distilbert_result:
    results.append(distilbert_result)

# ===== å¯¹æ¯”ç»“æœ =====
if len(results) == 2:
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ")
    print("=" * 80)
    
    # é¢„æµ‹ç»“æœå¯¹æ¯”
    print("\nğŸ¯ é¢„æµ‹ç»“æœå¯¹æ¯”:")
    for result in results:
        print(f"  {result['predicted_emoji']} {result['model_name']}: {result['predicted_label']} {result['predicted_color']} (ç½®ä¿¡åº¦: {result['confidence']:.4f})")
    
    # æ€§èƒ½å¯¹æ¯”
    print("\nâš¡ æ€§èƒ½å¯¹æ¯”:")
    for result in results:
        print(f"  ğŸš€ {result['model_name']}: {result['avg_time']:.2f} ms/æ¬¡ ({1000/result['avg_time']:.1f} æ¬¡/ç§’)")
    
    # é€Ÿåº¦å·®å¼‚
    if results[0]['avg_time'] != results[1]['avg_time']:
        faster_idx = 0 if results[0]['avg_time'] < results[1]['avg_time'] else 1
        slower_idx = 1 - faster_idx
        speedup = results[slower_idx]['avg_time'] / results[faster_idx]['avg_time']
        print(f"  ğŸ“ˆ {results[faster_idx]['model_name']} æ¯” {results[slower_idx]['model_name']} å¿« {speedup:.2f}x")
    
    # è¯¦ç»†æ¦‚ç‡åˆ†å¸ƒå¯¹æ¯”
    print("\nğŸ“Š è¯¦ç»†æ¦‚ç‡åˆ†å¸ƒå¯¹æ¯”:")
    for result in results:
        print(f"\n  {result['model_name']} æ¨¡å‹:")
        for i, (label, prob) in enumerate(zip(["è´Ÿé¢", "ä¸­æ€§", "æ­£é¢"], result['probabilities'])):
            emoji = label2emoji[label]
            color = label2color[label]
            bar_length = int(prob * 30)
            bar = "â–ˆ" * bar_length + "â–‘" * (30 - bar_length)
            marker = " â† é¢„æµ‹" if i == np.argmax(result['probabilities']) else ""
            print(f"    {emoji} {label} {color}: {bar} {prob:.4f} ({prob*100:.2f}%){marker}")
    
    # Logitså¯¹æ¯”
    print("\nğŸ”¢ åŸå§‹Logitså¯¹æ¯”:")
    for result in results:
        logits_str = ", ".join([f"{x:.4f}" for x in result['logits']])
        print(f"  {result['model_name']}: [{logits_str}]")
    
    # Tokenæ•°é‡å¯¹æ¯”
    print("\nğŸ“ Tokenå¤„ç†å¯¹æ¯”:")
    for result in results:
        print(f"  {result['model_name']}: {result['token_count']} tokens")
    
    # ä¸€è‡´æ€§æ£€æŸ¥
    print("\nğŸ” é¢„æµ‹ä¸€è‡´æ€§:")
    if results[0]['predicted_label'] == results[1]['predicted_label']:
        print("  âœ… ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹ç»“æœä¸€è‡´")
    else:
        print("  âš ï¸ ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹ç»“æœä¸ä¸€è‡´")
        conf_diff = abs(results[0]['confidence'] - results[1]['confidence'])
        print(f"  ğŸ“Š ç½®ä¿¡åº¦å·®å¼‚: {conf_diff:.4f}")

else:
    print("\nâŒ æ— æ³•å®Œæˆå¯¹æ¯”ï¼Œéƒ¨åˆ†æ¨¡å‹åŠ è½½å¤±è´¥")

print("\n" + "=" * 80)
print("ğŸ‰ æ¨¡å‹å¯¹æ¯”æµ‹è¯•å®Œæˆï¼")
print("=" * 80)