import torch
import torch.onnx
from transformers import BertTokenizer, BertForSequenceClassification
import os

def convert_finetuned_to_onnx():
    """å°†å¾®è°ƒåçš„TinyBERTæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼"""
    
    # ===== 1. æ£€æŸ¥å¾®è°ƒåçš„æ¨¡å‹æ˜¯å¦å­˜åœ¨ =====
    model_path = "./TinyBERT/tinybert_emotion_finetuned"
    if not os.path.exists(model_path):
        print("âŒ æœªæ‰¾åˆ°å¾®è°ƒåçš„æ¨¡å‹")
        print("è¯·å…ˆè¿è¡Œ finetune_tinybert.py è¿›è¡Œæ¨¡å‹å¾®è°ƒ")
        return False
    
    # ===== 2. åŠ è½½å¾®è°ƒåçš„æ¨¡å‹å’Œtokenizer =====
    print("ğŸ“¥ åŠ è½½å¾®è°ƒåçš„æ¨¡å‹...")
    try:
        tokenizer = BertTokenizer.from_pretrained(model_path)
        model = BertForSequenceClassification.from_pretrained(model_path)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False
    
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # ===== 3. å‡†å¤‡è™šæ‹Ÿè¾“å…¥ =====
    print("ğŸ”§ å‡†å¤‡è™šæ‹Ÿè¾“å…¥æ•°æ®...")
    dummy_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
    max_length = 128
    
    # ä½¿ç”¨tokenizerå¤„ç†è™šæ‹Ÿè¾“å…¥
    inputs = tokenizer(
        dummy_text,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=max_length
    )
    
    dummy_input_ids = inputs['input_ids']
    dummy_attention_mask = inputs['attention_mask']
    dummy_token_type_ids = inputs['token_type_ids']
    
    # ===== 4. å®šä¹‰è¾“å…¥å’Œè¾“å‡ºåç§° =====
    input_names = ['input_ids', 'attention_mask', 'token_type_ids']
    output_names = ['logits']
    
    # ===== 5. å®šä¹‰åŠ¨æ€è½´ï¼ˆæ”¯æŒä¸åŒåºåˆ—é•¿åº¦å’Œæ‰¹æ¬¡å¤§å°ï¼‰ =====
    dynamic_axes = {
        'input_ids': {0: 'batch_size', 1: 'sequence_length'},
        'attention_mask': {0: 'batch_size', 1: 'sequence_length'},
        'token_type_ids': {0: 'batch_size', 1: 'sequence_length'},
        'logits': {0: 'batch_size'}
    }
    
    # ===== 6. å¯¼å‡ºONNXæ¨¡å‹ =====
    onnx_path = "tinybert_emotion_finetuned.onnx"
    print(f"ğŸš€ å¼€å§‹å¯¼å‡ºONNXæ¨¡å‹åˆ° {onnx_path}...")
    
    try:
        torch.onnx.export(
            model,
            (dummy_input_ids, dummy_attention_mask, dummy_token_type_ids),
            onnx_path,
            export_params=True,
            opset_version=14,  # ä½¿ç”¨opset 14ä»¥æ”¯æŒæ›´å¤šæ“ä½œ
            do_constant_folding=True,
            input_names=input_names,
            output_names=output_names,
            dynamic_axes=dynamic_axes,
            verbose=False
        )
        print("âœ… ONNXæ¨¡å‹å¯¼å‡ºæˆåŠŸï¼")
        
        # ===== 7. éªŒè¯ONNXæ¨¡å‹ =====
        print("ğŸ” éªŒè¯ONNXæ¨¡å‹...")
        import onnxruntime as ort
        
        # åŠ è½½ONNXæ¨¡å‹
        session = ort.InferenceSession(onnx_path, providers=["CPUExecutionProvider"])
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        onnx_inputs = {
            'input_ids': dummy_input_ids.numpy(),
            'attention_mask': dummy_attention_mask.numpy(),
            'token_type_ids': dummy_token_type_ids.numpy()
        }
        
        # è¿è¡Œæ¨ç†
        onnx_outputs = session.run(None, onnx_inputs)
        
        # ä¸PyTorchæ¨¡å‹è¾“å‡ºå¯¹æ¯”
        with torch.no_grad():
            pytorch_outputs = model(dummy_input_ids, dummy_attention_mask, dummy_token_type_ids)
        
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸€è‡´
        pytorch_logits = pytorch_outputs.logits.numpy()
        onnx_logits = onnx_outputs[0]
        
        max_diff = abs(pytorch_logits - onnx_logits).max()
        print(f"ğŸ“Š PyTorchä¸ONNXè¾“å‡ºæœ€å¤§å·®å¼‚: {max_diff:.6f}")
        
        if max_diff < 1e-4:
            print("âœ… ONNXæ¨¡å‹éªŒè¯é€šè¿‡ï¼")
        else:
            print("âš ï¸  ONNXæ¨¡å‹è¾“å‡ºä¸PyTorchæœ‰è¾ƒå¤§å·®å¼‚")
        
        # ===== 8. è¾“å‡ºæ¨¡å‹ä¿¡æ¯ =====
        print("\nğŸ“‹ ONNXæ¨¡å‹ä¿¡æ¯:")
        print(f"  - æ–‡ä»¶è·¯å¾„: {onnx_path}")
        print(f"  - æ–‡ä»¶å¤§å°: {os.path.getsize(onnx_path) / (1024*1024):.2f} MB")
        print(f"  - è¾“å…¥æ•°é‡: {len(session.get_inputs())}")
        print(f"  - è¾“å‡ºæ•°é‡: {len(session.get_outputs())}")
        
        for i, input_info in enumerate(session.get_inputs()):
            print(f"  - è¾“å…¥{i+1}: {input_info.name}, å½¢çŠ¶: {input_info.shape}, ç±»å‹: {input_info.type}")
        
        for i, output_info in enumerate(session.get_outputs()):
            print(f"  - è¾“å‡º{i+1}: {output_info.name}, å½¢çŠ¶: {output_info.shape}, ç±»å‹: {output_info.type}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ONNXå¯¼å‡ºå¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸ¯ TinyBERTæƒ…æ„Ÿåˆ†ææ¨¡å‹ - å¾®è°ƒç‰ˆæœ¬è½¬ONNX")
    print("=" * 50)
    
    success = convert_finetuned_to_onnx()
    
    if success:
        print("\nğŸ‰ è½¬æ¢å®Œæˆï¼")
        print("ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨å¾®è°ƒåçš„ONNXæ¨¡å‹è¿›è¡Œé«˜ç²¾åº¦æƒ…æ„Ÿåˆ†æäº†ï¼")
    else:
        print("\nğŸ’¡ æç¤º:")
        print("1. è¯·å…ˆè¿è¡Œ 'python finetune_tinybert.py' è¿›è¡Œæ¨¡å‹å¾®è°ƒ")
        print("2. ç¡®ä¿å·²å®‰è£…æ‰€éœ€ä¾èµ–: pip install torch transformers onnxruntime")